{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhang/arangodb-cugraph-hackathon/blob/master/Top_Physics_Researchers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "qYJkhQk5szjl"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "rtB9iNqn_KPN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4nmLCFf3HoC",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Step 0: Package Installation & setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pV0dx8Ny1q64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe11a3a-f56b-47ed-fd1e-e69d4dbfa61e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nx-arangodb in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: networkx<=3.4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from nx-arangodb) (3.4)\n",
            "Requirement already satisfied: phenolrs~=0.5 in /usr/local/lib/python3.11/dist-packages (from nx-arangodb) (0.5.9)\n",
            "Requirement already satisfied: python-arango~=8.1 in /usr/local/lib/python3.11/dist-packages (from nx-arangodb) (8.1.6)\n",
            "Requirement already satisfied: adbnx-adapter~=5.0.5 in /usr/local/lib/python3.11/dist-packages (from nx-arangodb) (5.0.6)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (2.32.3)\n",
            "Requirement already satisfied: rich>=12.5.1 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (13.9.4)\n",
            "Requirement already satisfied: setuptools>=45 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (75.1.0)\n",
            "Requirement already satisfied: numpy~=1.26 in /usr/local/lib/python3.11/dist-packages (from phenolrs~=0.5->nx-arangodb) (1.26.4)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (2.3.0)\n",
            "Requirement already satisfied: requests_toolbelt in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (1.0.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (2.10.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (8.6.1)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (24.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.7.1->python-arango~=8.1->nx-arangodb) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# 1. Install nx-arangodb via pip\n",
        "# Github: https://github.com/arangodb/nx-arangodb\n",
        "\n",
        "!pip install nx-arangodb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOa19V6Tszjr",
        "outputId": "1100c94d-32a0-4730-b2e0-5f6a5c44c3ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar  9 20:33:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "# 2. Check if you have an NVIDIA GPU\n",
        "# Note: If this returns \"command not found\", then GPU-based algorithms via cuGraph are unavailable\n",
        "\n",
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1MHrHTWszjr",
        "outputId": "43195307-6fea-41be-d2eb-aa2be5405209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Requirement already satisfied: nx-cugraph-cu12 in /usr/local/lib/python3.11/dist-packages (25.2.0)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (13.3.0)\n",
            "Requirement already satisfied: networkx>=3.2 in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (3.4)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (1.26.4)\n",
            "Requirement already satisfied: pylibcugraph-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (25.2.0)\n",
            "Requirement already satisfied: libcugraph-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==25.2.*->nx-cugraph-cu12) (25.2.0)\n",
            "Requirement already satisfied: pylibraft-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==25.2.*->nx-cugraph-cu12) (25.2.0)\n",
            "Requirement already satisfied: rmm-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==25.2.*->nx-cugraph-cu12) (25.2.0)\n",
            "Requirement already satisfied: libraft-cu12==25.2.* in /usr/local/lib/python3.11/dist-packages (from libcugraph-cu12==25.2.*->pylibcugraph-cu12==25.2.*->nx-cugraph-cu12) (25.2.0)\n",
            "Requirement already satisfied: cuda-python<13.0a0,>=12.6.2 in /usr/local/lib/python3.11/dist-packages (from pylibraft-cu12==25.2.*->pylibcugraph-cu12==25.2.*->nx-cugraph-cu12) (12.6.2.post1)\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.11/dist-packages (from libraft-cu12==25.2.*->libcugraph-cu12==25.2.*->pylibcugraph-cu12==25.2.*->nx-cugraph-cu12) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-curand-cu12 in /usr/local/lib/python3.11/dist-packages (from libraft-cu12==25.2.*->libcugraph-cu12==25.2.*->pylibcugraph-cu12==25.2.*->nx-cugraph-cu12) (10.3.6.82)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12 in /usr/local/lib/python3.11/dist-packages (from libraft-cu12==25.2.*->libcugraph-cu12==25.2.*->pylibcugraph-cu12==25.2.*->nx-cugraph-cu12) (11.6.3.83)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.11/dist-packages (from libraft-cu12==25.2.*->libcugraph-cu12==25.2.*->pylibcugraph-cu12==25.2.*->nx-cugraph-cu12) (12.5.1.3)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x>=12.0.0->nx-cugraph-cu12) (0.8.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12->libraft-cu12==25.2.*->libcugraph-cu12==25.2.*->pylibcugraph-cu12==25.2.*->nx-cugraph-cu12) (12.5.82)\n"
          ]
        }
      ],
      "source": [
        "# 3. Install nx-cugraph via pip\n",
        "# Note: Only enable this installation if the step above is working!\n",
        "\n",
        "!pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6bTR9bMszjr",
        "outputId": "d04d0857-922d-401e-d2e6-bf068b561e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.5)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.0.11)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.43)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.18)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.55)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.16)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.25.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "# 4. Install LangChain & LangGraph\n",
        "\n",
        "!pip install --upgrade langchain langchain-community langchain-openai langgraph langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIaTDaaB7Emb",
        "outputId": "b60cd70c-a22b-402b-a238-0154c7e99680"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC3dR93RdoTV",
        "outputId": "b96f5122-cd78-4867-ce7f-c01474a8cf1b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.20.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.11)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.7.2)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.9.10)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cUkcguDKszjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11adc28-c961-4484-ad91-fd92f712996b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[20:33:53 +0000] [INFO]: NetworkX-cuGraph is available.\n",
            "INFO:nx_arangodb:NetworkX-cuGraph is available.\n"
          ]
        }
      ],
      "source": [
        "# 5. Import the required modules\n",
        "\n",
        "import networkx as nx\n",
        "import nx_arangodb as nxadb\n",
        "\n",
        "from arango import ArangoClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import geopandas as gpd\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.graphs import ArangoGraph\n",
        "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
        "from langchain_core.tools import tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B2M3QHNszjs",
        "outputId": "495d8a25-78e0-421d-b7f6-a4adb09ac15a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<StandardDatabase _system>\n"
          ]
        }
      ],
      "source": [
        "# 6. Connect to the ArangoDB database\n",
        "from google.colab import userdata\n",
        "\n",
        "myarangohost = userdata.get('ARANGO_HOST')\n",
        "mypass = userdata.get('ARANGO_PASS')\n",
        "db = ArangoClient(hosts=myarangohost).db(username=\"root\", password=mypass, verify=True)\n",
        "\n",
        "print(db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "dfo8kas-szjs"
      },
      "source": [
        "## Step 1: Choose & prepare your dataset for NetworkX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFtt53_yszjs"
      },
      "source": [
        "This section will provide a template for data transformation, including placeholders for loading CSV/JSON files, defining graph schemas, and preparing data for ingestion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "dO03dsi-szjt"
      },
      "source": [
        "## Step 2: Convert and Load Graph Data into NetworkX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u04QV--0szjt"
      },
      "source": [
        "Here, we load the data of physics research papers and their authors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QffIKL6pszju"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "G = nx.read_graphml(\"coauthorship_bipartite.graphml\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now check out one of the physicists and their papers and co-authors"
      ],
      "metadata": {
        "id": "obEvK_OM0Rpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_author_network(G, author_id):\n",
        "  try:\n",
        "    sub_nodes = {author_id} | set(G.neighbors(author_id))\n",
        "\n",
        "    print(f\"{sub_nodes=}\")\n",
        "\n",
        "    return G.subgraph(sub_nodes)\n",
        "  except:\n",
        "    print(f\"Author {author_id} not found\")\n",
        "    return None\n",
        "\n",
        "import networkx as nx\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as pyo\n",
        "\n",
        "def plot_author_network(G, author_id):\n",
        "  G_author = get_author_network(G, author_id)\n",
        "  pos = nx.spring_layout(G_author, seed=42)\n",
        "\n",
        "  # Prepare lists for nodes separated by type.\n",
        "  authors_x, authors_y, authors_text = [], [], []\n",
        "  papers_x, papers_y, papers_text = [], [], []\n",
        "\n",
        "  for node, data in G_author.nodes(data=True):\n",
        "      x, y = pos[node]\n",
        "      if data.get('type') == 'Author':\n",
        "          authors_x.append(x)\n",
        "          authors_y.append(y)\n",
        "          # Display the author's name as the label.\n",
        "          authors_text.append(data.get('name', str(node)))\n",
        "      else:\n",
        "          papers_x.append(x)\n",
        "          papers_y.append(y)\n",
        "          # Display the paper title as the label.\n",
        "          papers_text.append(data.get('title', str(node)))\n",
        "\n",
        "  # Create the author trace with distinct marker (circle) and color.\n",
        "  authors_trace = go.Scatter(\n",
        "      x=authors_x,\n",
        "      y=authors_y,\n",
        "      mode='markers+text',\n",
        "      name='Authors',\n",
        "      text=authors_text,\n",
        "      textposition=\"top center\",\n",
        "      marker=dict(symbol=\"circle\", size=20, color=\"skyblue\"),\n",
        "      hoverinfo=\"text\"\n",
        "  )\n",
        "\n",
        "  # Create the paper trace with a different marker (square) and color.\n",
        "  papers_trace = go.Scatter(\n",
        "      x=papers_x,\n",
        "      y=papers_y,\n",
        "      mode='markers+text',\n",
        "      name='Papers',\n",
        "      text=papers_text,\n",
        "      textposition=\"bottom center\",\n",
        "      marker=dict(symbol=\"square\", size=20, color=\"lightgreen\"),\n",
        "      hoverinfo=\"text\"\n",
        "  )\n",
        "\n",
        "  # Build edge traces for all edges.\n",
        "  edge_x = []\n",
        "  edge_y = []\n",
        "  for source, target, data in G_author.edges(data=True):\n",
        "      x0, y0 = pos[source]\n",
        "      x1, y1 = pos[target]\n",
        "      edge_x.extend([x0, x1, None])\n",
        "      edge_y.extend([y0, y1, None])\n",
        "\n",
        "  edge_trace = go.Scatter(\n",
        "      x=edge_x,\n",
        "      y=edge_y,\n",
        "      line=dict(width=1, color='#888'),\n",
        "      hoverinfo='none',\n",
        "      mode='lines'\n",
        "  )\n",
        "\n",
        "  # Create the Plotly figure.\n",
        "  fig = go.Figure(\n",
        "      data=[edge_trace, authors_trace, papers_trace],\n",
        "      layout=go.Layout(\n",
        "          title='Author Ego Network',\n",
        "          titlefont_size=16,\n",
        "          showlegend=True,\n",
        "          hovermode='closest',\n",
        "          margin=dict(b=20, l=5, r=5, t=40),\n",
        "          xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "          yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Display the interactive visualization.\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UIu76aJy0Q_f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_author_network(G, \"ur.011344733521.27\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "YbE73kYO7QeL",
        "outputId": "7803a206-b9df-40fb-aa0f-0e4fe61faf30"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sub_nodes={'ur.0762271037.55', 'ur.07353444575.53', '10.1002/CPA.20124', 'ur.011344733521.27'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"a3509b6c-4cd1-409e-9794-e67b1538ad95\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a3509b6c-4cd1-409e-9794-e67b1538ad95\")) {                    Plotly.newPlot(                        \"a3509b6c-4cd1-409e-9794-e67b1538ad95\",                        [{\"hoverinfo\":\"none\",\"line\":{\"color\":\"#888\",\"width\":1},\"mode\":\"lines\",\"x\":[0.3563184624919795,-0.3563184624919799,null,0.3563184624919795,-0.993928038388634,null,0.3563184624919795,0.9939280383886342,null,0.9939280383886342,-0.3563184624919799,null,0.9939280383886342,-0.993928038388634,null,-0.3563184624919799,-0.993928038388634,null],\"y\":[1.0,-1.0,null,1.0,0.3541549104663071,null,1.0,-0.35415491046630787,null,-0.35415491046630787,-1.0,null,-0.35415491046630787,0.3541549104663071,null,-1.0,0.3541549104663071,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"skyblue\",\"size\":20,\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"Authors\",\"text\":[\"Emmanuel J Cand√®s\",\"Justin K Romberg\",\"Terence Tao\"],\"textposition\":\"top center\",\"x\":[0.3563184624919795,0.9939280383886342,-0.993928038388634],\"y\":[1.0,-0.35415491046630787,0.3541549104663071],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":\"lightgreen\",\"size\":20,\"symbol\":\"square\"},\"mode\":\"markers+text\",\"name\":\"Papers\",\"text\":[\"Sparse Signal Recovery via L1-Regularization and Matrix Uncertainty\"],\"textposition\":\"bottom center\",\"x\":[-0.3563184624919799],\"y\":[-1.0],\"type\":\"scatter\"}],                        {\"hovermode\":\"closest\",\"margin\":{\"b\":20,\"l\":5,\"r\":5,\"t\":40},\"showlegend\":true,\"title\":{\"font\":{\"size\":16},\"text\":\"Author Ego Network\"},\"xaxis\":{\"showgrid\":false,\"showticklabels\":false,\"zeroline\":false},\"yaxis\":{\"showgrid\":false,\"showticklabels\":false,\"zeroline\":false},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a3509b6c-4cd1-409e-9794-e67b1538ad95');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "jAvuBFEEszjv"
      },
      "source": [
        "## Step 3: Persist the Graph in ArangoDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvXd6Q--szjv"
      },
      "source": [
        "As in the template, we persist the graph inside ArangoDB.  \n",
        "\n",
        "\n",
        "#### Bad Data Handling\n",
        "I noticed that some of my data is \"bad\" in the sense that there are nan values in some columns.  Tools like networkx and gephi can handle this, but  ArangoDB seems not to like that, and rolls back the whole transaction (seemingly).  Therefore, I clean up by replacing 'country', 'abstract', id, and 'year' with default values when they are nan"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "for node, data in G.nodes(data=True):\n",
        "    val = data.get('abstract', None)\n",
        "\n",
        "    if val is None or (isinstance(val, float) and math.isnan(val)):\n",
        "        data['abstract'] = \"\"\n",
        "\n",
        "    countryval = data.get('country', None)\n",
        "\n",
        "    if countryval is None or (isinstance(countryval, float) and math.isnan(countryval)):\n",
        "        data['country'] = \"UNKNOWN\"\n",
        "\n",
        "\n",
        "    year = data.get('year', None)\n",
        "\n",
        "    # For the authors, the year is nan, but just to get past the problem,\n",
        "    # i set it to year 0\n",
        "    if year is None or (isinstance(year, float) and math.isnan(year)):\n",
        "        data['year'] = 0\n",
        "\n",
        "\n",
        "    gender = data.get('gender', None)\n",
        "\n",
        "    if gender is None or (isinstance(gender, float) and math.isnan(gender)):\n",
        "        data['gender'] = \"male\""
      ],
      "metadata": {
        "id": "PuIpPODE0HEF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edge_definitions = [\n",
        "    {\n",
        "        \"edge_collection\": \"written_by\",\n",
        "        \"from_vertex_collections\": [\"Paper\"],\n",
        "        \"to_vertex_collections\": [\"Author\"],\n",
        "    },\n",
        "    {\n",
        "        \"edge_collection\": \"co_author_with\",\n",
        "        \"from_vertex_collections\": [\"Author\"],\n",
        "        \"to_vertex_collections\": [\"Author\"],\n",
        "    }\n",
        "]\n",
        "\n",
        "from adbnx_adapter import ADBNX_Controller, ADBNX_Adapter\n",
        "\n",
        "class Custom_ADBNX_Controller(ADBNX_Controller):\n",
        "    \"\"\"Custom controller to map heterogeneous nodes and edges.\"\"\"\n",
        "\n",
        "    def _identify_networkx_node(self, nx_node_id, nx_node, adb_v_cols):\n",
        "        \"\"\"Determine the correct ArangoDB collection for a given node.\"\"\"\n",
        "        if \"type\" in nx_node:\n",
        "            return nx_node[\"type\"]  # Returns either \"Author\" or \"Paper\"\n",
        "        raise ValueError(f\"Node {nx_node_id} does not have a 'type' attribute.\")\n",
        "\n",
        "    def _identify_networkx_edge(self, nx_edge, from_node_id, to_node_id, nx_map, adb_e_cols):\n",
        "        \"\"\"Determine the correct ArangoDB edge collection based on the 'relation' key.\"\"\"\n",
        "        relation = nx_edge.get(\"relation\", None)\n",
        "        if relation == \"WRITTEN_BY\":\n",
        "            return \"written_by\"\n",
        "        elif relation == \"CO_AUTHOR_WITH\":\n",
        "            return \"co_author_with\"\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown edge relation: {relation}\")\n",
        "\n",
        "    def _keyify_networkx_node(self, i, nx_node_id, nx_node, col):\n",
        "        \"\"\"Generate valid ArangoDB _key values.\"\"\"\n",
        "        key = re.sub(r'[^a-zA-Z0-9_-]', '_', nx_node_id)\n",
        "        return str(key)\n",
        "\n",
        "# Initialize ArangoDB Adapter\n",
        "custom_adbnx_adapter = ADBNX_Adapter(db, Custom_ADBNX_Controller())\n",
        "\n",
        "# Define Graph Name\n",
        "graph_name = \"physics\"\n",
        "\n",
        "# Delete existing graph (optional)\n",
        "db.delete_graph(graph_name, drop_collections=True, ignore_missing=True)\n",
        "\n",
        "# Convert NetworkX Graph to ArangoDB\n",
        "adb_g = custom_adbnx_adapter.networkx_to_arangodb(graph_name, G, edge_definitions)\n",
        "\n",
        "# Verify the graph structure\n",
        "print(db.graph(graph_name).edge_definitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192,
          "referenced_widgets": [
            "1c0422e3e07b4921ae79bc629d69bfc1",
            "af9e3e491ee1457f9ca06b81b8cb8c33",
            "6ae5e833f4874511a8cdd1b1b3522873",
            "97664723412c475c82d0cc663aee079b"
          ]
        },
        "id": "7ODzkEv5GPS2",
        "outputId": "2bc2b9cb-3f09-477d-8f60-1086308aff94"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2025/03/09 20:33:58 +0000] [16579] [INFO] - adbnx_adapter: Instantiated ADBNX_Adapter with database '_system'\n",
            "INFO:adbnx_adapter:Instantiated ADBNX_Adapter with database '_system'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c0422e3e07b4921ae79bc629d69bfc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ae5e833f4874511a8cdd1b1b3522873"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2025/03/09 20:34:02 +0000] [16579] [INFO] - adbnx_adapter: Created ArangoDB 'physics' Graph\n",
            "INFO:adbnx_adapter:Created ArangoDB 'physics' Graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'edge_collection': 'co_author_with', 'from_vertex_collections': ['Author'], 'to_vertex_collections': ['Author']}, {'edge_collection': 'written_by', 'from_vertex_collections': ['Paper'], 'to_vertex_collections': ['Author']}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Abstract Search View\n",
        "ArangoDB is a multi-model database, which means we are able to leverage full-text search on our data in addition to graph queries.  For that, I am creating a \"search view\" to search through the text of the abstracts"
      ],
      "metadata": {
        "id": "eGtg6zHYSPT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ArangoSearch View\n",
        "view_name = \"abstract_search_view\"\n",
        "\n",
        "# If it already exists, delete it\n",
        "try:\n",
        "    db.delete_view(view_name)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create a new ArangoSearch View\n",
        "db.create_view(view_name, view_type=\"arangosearch\", properties={})\n",
        "\n",
        "print(f\"Created ArangoSearch view: {view_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PezfwQDShhy",
        "outputId": "1675a71c-4248-425f-8d61-34f6a5186e9c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created ArangoSearch view: abstract_search_view\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ArangoSearch properties (indexes for searching)\n",
        "view_properties = {\n",
        "    \"links\": {\n",
        "        \"Paper\": {  # Index Paper collection\n",
        "            \"fields\": {\n",
        "                \"abstract\": {\"analyzers\": [\"text_en\"]}\n",
        "            }\n",
        "        },\n",
        "        \"Author\": {  # Index Author collection\n",
        "            \"fields\": {\n",
        "                \"name\": {\"analyzers\": [\"text_en\"]}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "db.update_view(view_name, view_properties)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5McNwscU7gy",
        "outputId": "83a68b9e-7415-4954-8f52-cb35a5644325"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'global_id': 'hBE61A6883233/212448',\n",
              " 'id': '212448',\n",
              " 'name': 'abstract_search_view',\n",
              " 'type': 'arangosearch',\n",
              " 'cleanup_interval_step': 2,\n",
              " 'commit_interval_msec': 1000,\n",
              " 'consolidation_interval_msec': 1000,\n",
              " 'consolidation_policy': {'type': 'tier',\n",
              "  'segments_min': 1,\n",
              "  'segments_max': 10,\n",
              "  'segments_bytes_max': 5368709120,\n",
              "  'segments_bytes_floor': 2097152,\n",
              "  'min_score': 0},\n",
              " 'primary_sort': [],\n",
              " 'primary_sort_compression': 'lz4',\n",
              " 'stored_values': [],\n",
              " 'writebuffer_idle': 64,\n",
              " 'writebuffer_active': 0,\n",
              " 'writebuffer_max_size': 33554432,\n",
              " 'links': {'Paper': {'analyzers': ['identity'],\n",
              "   'fields': {'abstract': {'analyzers': ['text_en']}},\n",
              "   'includeAllFields': False,\n",
              "   'storeValues': 'none',\n",
              "   'trackListPositions': False},\n",
              "  'Author': {'analyzers': ['identity'],\n",
              "   'fields': {'name': {'analyzers': ['text_en']}},\n",
              "   'includeAllFields': False,\n",
              "   'storeValues': 'none',\n",
              "   'trackListPositions': False}},\n",
              " 'optimizeTopK': []}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can add some niceties like stemming and lemmatization to make the searches a bit nicer"
      ],
      "metadata": {
        "id": "Yr4RxiCDYbDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db.create_analyzer(\n",
        "    name=\"stem_en\",\n",
        "    analyzer_type=\"stem\",\n",
        "    properties={ \"locale\": \"en\" },  # English stemming\n",
        "    features=[\"frequency\", \"norm\", \"position\"]  # Required features\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPD_Xd6vYhNA",
        "outputId": "daba1acb-c967-43f7-e1f4-3b7d6d21aaa6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': '_system::stem_en',\n",
              " 'type': 'stem',\n",
              " 'properties': {'locale': 'en'},\n",
              " 'features': ['frequency', 'position', 'norm']}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Author Search View\n",
        "It is useful to allow one to search for authors easily without knowing the exact spelling etc"
      ],
      "metadata": {
        "id": "9VSGzZ_c5Cr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ArangoSearch View\n",
        "view_name = \"author_search_view\"\n",
        "\n",
        "# If it already exists, delete it\n",
        "try:\n",
        "    db.delete_view(view_name)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create a new ArangoSearch View\n",
        "db.create_view(view_name, view_type=\"arangosearch\", properties={})\n",
        "\n",
        "print(f\"Created ArangoSearch view: {view_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZfc_rpD5U5J",
        "outputId": "85f0018d-5c3e-4f83-b461-d8fa911e568d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created ArangoSearch view: author_search_view\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW6u0fBcszjv"
      },
      "source": [
        "We now have the graph inside arangoDB, and can now load it from the db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O3ThlpALI5G9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdba7441-908b-4981-d4c0-8298899a1a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[20:34:04 +0000] [INFO]: Graph 'physics' exists.\n",
            "INFO:nx_arangodb:Graph 'physics' exists.\n",
            "[20:34:05 +0000] [INFO]: Default node type set to 'Author'\n",
            "INFO:nx_arangodb:Default node type set to 'Author'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph named 'physics' with 2840 nodes and 5484 edges\n"
          ]
        }
      ],
      "source": [
        "# 2. Re-connect to the same Graph\n",
        "\n",
        "G_adb = nxadb.Graph(name=\"physics\", db=db)\n",
        "\n",
        "print(G_adb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "eQtlbMb0szjx"
      },
      "source": [
        "## Step 4: Build the Agentic App with LangChain & LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGNNDN3sszjx"
      },
      "source": [
        "Here, I finally start building the agentic app.  I'm targetting the following types of questions:\n",
        "\n",
        "\n",
        "- Find papers talking about a topic, and see the demographics (country of authors, affiliations of authors, time series of years) and graph statistics about that\n",
        "- Influential Authors and gender fairness in network ranking\n",
        "- Predict new collaborations\n",
        "- Degree of separation between two authors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jJlbLFaNszjx"
      },
      "outputs": [],
      "source": [
        "# 1. Create the ArangoGraph LangChain wrapper\n",
        "# Reference: https://api.python.langchain.com/en/latest/graphs/langchain_community.graphs.arangodb_graph.ArangoGraph.html\n",
        "\n",
        "arango_graph = ArangoGraph(db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYu0nggFszjx",
        "outputId": "3ecc9d09-a23c-453c-96e7-39b110a3b232"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-c234d261-f2c3-4bbe-b5f3-6a991497d17e-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 2. Define the llm object\n",
        "# Note: You can use any llm you want. We will be using OpenAI for example purposes.\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "llm.invoke(\"hello!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def favourite_person(query: str):\n",
        "    \"\"\"You are responsible for responding to being asked who your favourite person is.\n",
        "    You must say Sudhang Shankar!\n",
        "    \"\"\"\n",
        "    return \"Sudhang Shankar!\""
      ],
      "metadata": {
        "id": "3R-Ih9JjP3Oz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bsILmx8wszjy"
      },
      "outputs": [],
      "source": [
        "# 4. Define the Text to AQL Tool\n",
        "# Reference: https://python.langchain.com/docs/integrations/graphs/arangodb/\n",
        "# Reference: https://python.langchain.com/api_reference/community/chains/langchain_community.chains.graph_qa.arangodb.ArangoGraphQAChain.html\n",
        "\n",
        "@tool\n",
        "def text_to_aql_to_text(query: str):\n",
        "    \"\"\"This tool is available to invoke the\n",
        "    ArangoGraphQAChain object, which enables you to\n",
        "    translate a Natural Language Query into AQL, execute\n",
        "    the query, and translate the result back into Natural Language.\n",
        "\n",
        "    If a query can be solved using AQL, prioritise it\n",
        "    \"\"\"\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "    chain = ArangoGraphQAChain.from_llm(\n",
        "    \tllm=llm,\n",
        "    \tgraph=arango_graph,\n",
        "    \tverbose=False,                # Setting False to reduce clutter\n",
        "      allow_dangerous_requests=True\n",
        "    )\n",
        "\n",
        "    chain.aql_examples = \"\"\"\n",
        "        # Example 1: Find papers written by Terence Tao\n",
        "          WITH Paper, Author, written_by\n",
        "          FOR author IN Author\n",
        "            FILTER author.name == \"Terence Tao\"\n",
        "            FOR paper IN INBOUND author written_by\n",
        "              RETURN paper\n",
        "\n",
        "        # Example 2: Find authors with whom Terence Tao has written papers\n",
        "          WITH Author, co_author_with\n",
        "          FOR author IN Author\n",
        "            FILTER author.name == \"Terence Tao\"\n",
        "            FOR coauthor IN 1..1 INBOUND author co_author_with\n",
        "              RETURN coauthor.name\n",
        "\n",
        "        # Example 3: With whom has Terence Tao worked on papers?\n",
        "          WITH Author, co_author_with\n",
        "          FOR author IN Author\n",
        "            FILTER author.name == \"Terence Tao\"\n",
        "            FOR coauthor IN 1..1 INBOUND author co_author_with\n",
        "              RETURN coauthor.name\n",
        "      \"\"\"\n",
        "\n",
        "    result = chain.invoke(query)\n",
        "\n",
        "    return str(result[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iVEaoy9gszjy"
      },
      "outputs": [],
      "source": [
        "# 5. Define the Text to NetworkX/cuGraph Tool\n",
        "# Note: It is encouraged to experiment and improve this section! This is just a placeholder:\n",
        "\n",
        "@tool\n",
        "def text_to_nx_algorithm_to_text(query):\n",
        "    \"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
        "    the ArangoDB Graph. You are responsible for accepting the\n",
        "    Natural Language Query, establishing which algorithm needs to\n",
        "    be executed, executing the algorithm, and translating the results back\n",
        "    to Natural Language, with respect to the original query.\n",
        "\n",
        "    If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
        "    this tool.\n",
        "    \"\"\"\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "    ######################\n",
        "    print(\"1) Generating NetworkX code\")\n",
        "\n",
        "    text_to_nx = llm.invoke(f\"\"\"\n",
        "    I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
        "\n",
        "    I have the following graph analysis query: {query}.\n",
        "\n",
        "    Generate the Python Code required to answer the query using the `G_adb` object.\n",
        "\n",
        "    Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n",
        "\n",
        "    Only assume that networkx is installed, and other base python dependencies.\n",
        "\n",
        "    Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n",
        "\n",
        "    Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n",
        "\n",
        "    Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n",
        "\n",
        "    Your code:\n",
        "    \"\"\").content\n",
        "\n",
        "    text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n",
        "\n",
        "    print('-'*10)\n",
        "    print(text_to_nx_cleaned)\n",
        "    print('-'*10)\n",
        "\n",
        "    ######################\n",
        "\n",
        "    print(\"\\n2) Executing NetworkX code\")\n",
        "    global_vars = {\"G_adb\": G_adb, \"nx\": nx}\n",
        "    local_vars = {}\n",
        "\n",
        "    try:\n",
        "        exec(text_to_nx_cleaned, global_vars, local_vars)\n",
        "        text_to_nx_final = text_to_nx\n",
        "    except Exception as e:\n",
        "        print(f\"EXEC ERROR: {e}\")\n",
        "        return f\"EXEC ERROR: {e}\"\n",
        "\n",
        "        # TODO: Consider experimenting with a code corrector!\n",
        "        attempt = 1\n",
        "        MAX_ATTEMPTS = 3\n",
        "\n",
        "        # while attempt <= MAX_ATTEMPTS\n",
        "            # ...\n",
        "\n",
        "    print('-'*10)\n",
        "    FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
        "    print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n",
        "    print('-'*10)\n",
        "\n",
        "    ######################\n",
        "\n",
        "    print(\"3) Formulating final answer\")\n",
        "\n",
        "    nx_to_text = llm.invoke(f\"\"\"\n",
        "        I have a NetworkX Graph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
        "\n",
        "        I have the following graph analysis query: {query}.\n",
        "\n",
        "        I have executed the following python code to help me answer my query:\n",
        "\n",
        "        ---\n",
        "        {text_to_nx_final}\n",
        "        ---\n",
        "\n",
        "        The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n",
        "\n",
        "        Based on my original Query and FINAL_RESULT, generate a short and concise response to\n",
        "        answer my query.\n",
        "\n",
        "        Your response:\n",
        "    \"\"\").content\n",
        "\n",
        "    return nx_to_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Create the Agentic Application\n",
        "\n",
        "tools = [\n",
        "  text_to_aql_to_text,\n",
        "  text_to_nx_algorithm_to_text,\n",
        "  favourite_person\n",
        "]\n",
        "\n",
        "\n",
        "def query_graph(query):\n",
        "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "    app = create_react_agent(llm, tools)\n",
        "    final_state = app.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
        "    return final_state[\"messages\"][-1].content"
      ],
      "metadata": {
        "id": "U002YEVOPyRf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_graph(\"How many authors are in the network\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "pPHZW3RPP8b2",
        "outputId": "ac511abf-9df1-4188-8496-b63ff2c71197"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
            "AQL Query (1):\u001b[32;1m\u001b[1;3m\n",
            "WITH Author\n",
            "FOR author IN Author\n",
            "  COLLECT WITH COUNT INTO length\n",
            "  RETURN length\n",
            "\u001b[0m\n",
            "AQL Result:\n",
            "\u001b[32;1m\u001b[1;3m[1740]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are 1,740 authors in the network.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_graph(\"How many papers are in the network\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "ZfBxwvY1Q0RP",
        "outputId": "7c588f81-9c79-4e05-838c-842d921639f2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
            "AQL Query (1):\u001b[32;1m\u001b[1;3m\n",
            "WITH Paper\n",
            "FOR paper IN Paper\n",
            "  COLLECT WITH COUNT INTO length\n",
            "RETURN length\n",
            "\u001b[0m\n",
            "AQL Result:\n",
            "\u001b[32;1m\u001b[1;3m[1100]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The network contains a total of 1,100 papers.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_graph(\"What papers did Terence Tao write?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "_A6zqtPcQ6rh",
        "outputId": "37a3da7a-f640-4fff-cb04-587c998aebfc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
            "AQL Query (1):\u001b[32;1m\u001b[1;3m\n",
            "WITH Paper, Author, written_by\n",
            "FOR author IN Author\n",
            "  FILTER author.name == \"Terence Tao\"\n",
            "  FOR paper IN INBOUND author written_by\n",
            "    RETURN paper\n",
            "\u001b[0m\n",
            "AQL Result:\n",
            "\u001b[32;1m\u001b[1;3m[{'_key': '10_1002_CPA_20124', '_id': 'Paper/10_1002_CPA_20124', '_rev': '_jVwbTwu--h', 'type': 'Paper', 'title': 'Sparse Signal Recovery via L1-Regularization and Matrix Uncertainty', 'year': 2005, 'num_citations': 6257, 'abstract': \"Suppose we wish to recover a vector x_0 –Ñ R^m (e.g., a digital signal or image) from incomplete and contaminated observations y = Ax_0 + e; A is an n by m matrix with far fewer rows than columns (n ¬´ m) and e is an error term. Is it possible to recover x_0 accurately based on the data y? \\nTo recover x_0, we consider the solution x^# to the l_(1-)regularization problem min ‚Äñx‚Äñl_1 subject to ‚ÄñAx - y‚Äñl(2) ‚â§ –Ñ, where –Ñ is the size of the error term e. We show that if A obeys a uniform uncertainty principle (with unit-normed columns) and if the vector x_0 is sufficiently sparse, then the solution is within the noise level ‚Äñx^# - x_0‚Äñl_2 ‚â§ C –Ñ. As a first example, suppose that A is a Gaussian random matrix; then stable recovery occurs for almost all such A's provided that the number of nonzeros of x_0 is of about the same order as the number of observations. As a second instance, suppose one observes few Fourier samples of x_0; then stable recovery occurs for almost any set of n coefficients provided that the number of nonzeros is of the order of n/[log m]^6. In the case where the error term vanishes, the recovery is of course exact, and this work actually provides novel insights into the exact recovery phenomenon discussed in earlier papers. The methodology also explains why one can also very nearly recover approximately sparse signals.\", 'country': 'UNKNOWN', 'gender': 'male'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Terence Tao authored the paper titled \"Sparse Signal Recovery via L1-Regularization and Matrix Uncertainty,\" published in 2005. This influential paper, cited 6,257 times, addresses the challenge of recovering a vector from incomplete and contaminated observations using L1-regularization. It delves into the conditions necessary for accurate recovery and offers insights into the exact recovery phenomenon, as well as methodologies for nearly recovering approximately sparse signals.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_graph(\"With whom has Terence Tao worked on papers?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "Jgu7UxB8SVxQ",
        "outputId": "ab0517cd-befd-4b43-8475-db3ea4bebcb4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
            "AQL Query (1):\u001b[32;1m\u001b[1;3m\n",
            "WITH Author, co_author_with\n",
            "FOR author IN Author\n",
            "  FILTER author.name == \"Terence Tao\"\n",
            "  FOR coauthor IN 1..1 INBOUND author co_author_with\n",
            "    RETURN coauthor.name\n",
            "\u001b[0m\n",
            "AQL Result:\n",
            "\u001b[32;1m\u001b[1;3m['Justin K Romberg', 'Emmanuel J Cand√®s']\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Terence Tao has worked on academic papers with Justin K. Romberg and Emmanuel J. Cand√®s.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Stats about a Topic\n",
        "\n",
        "We allow the user to input some physics-related topic, and then search our abstracts for that topic.  If we find it, we get the following information about the papers:\n",
        "- Who worked on these topics\n",
        "- Where in the world these authors are\n",
        "- What are their affiliations\n",
        "- What are the trends for this topic over time\n",
        "- How influential are the authors who worked on this topic"
      ],
      "metadata": {
        "id": "LRDLCV0TFyPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tools for the dashboard"
      ],
      "metadata": {
        "id": "gy5LyAvwoRHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The following is public domain, so it is okay to use\n",
        "WORLD_MAP_URL = \"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\"\n",
        "world = gpd.read_file(WORLD_MAP_URL)\n",
        "\n",
        "# Tool: Search Papers Based on Topic\n",
        "def search_papers(search_term: str):\n",
        "    \"\"\"\n",
        "    Finds papers matching a research topic using full-text search in ArangoDB.\n",
        "    \"\"\"\n",
        "\n",
        "    # Using PHRASE search gives better results than FILTER, which\n",
        "    # is what I was trying before\n",
        "    query = \"\"\"\n",
        "    WITH Paper\n",
        "    FOR doc IN abstract_search_view\n",
        "    SEARCH ANALYZER(\n",
        "        PHRASE(doc.abstract, @search_term),\n",
        "        \"text_en\"\n",
        "    )\n",
        "    RETURN { id: doc._id, title: doc.title, year: doc.year }\n",
        "    \"\"\"\n",
        "\n",
        "    results = list(db.aql.execute(query, bind_vars={\"search_term\": f\"%{search_term}%\"}))\n",
        "    return results\n",
        "\n",
        "# Tool: Find Authors of Those Papers\n",
        "def get_authors_for_papers(paper_ids):\n",
        "    \"\"\"\n",
        "    Finds authors of the given papers and their demographics.\n",
        "    \"\"\"\n",
        "    query = \"\"\"\n",
        "    WITH written_by, Author\n",
        "    FOR e IN written_by\n",
        "    FILTER e._from IN @paper_ids\n",
        "    FOR a IN Author\n",
        "    FILTER a._id == e._to\n",
        "    RETURN { name: a.name, country: a.country, affiliation: a.affiliation, paper_id: e._from }\n",
        "\n",
        "    \"\"\"\n",
        "    results = list(db.aql.execute(query, bind_vars={\"paper_ids\": paper_ids}))\n",
        "    return results\n",
        "\n",
        "# Tool: visualise plots about authors and papers\n",
        "def generate_plots(author_data, paper_data):\n",
        "    \"\"\"\n",
        "    Creates interactive Plotly plots for:\n",
        "    1. Country distribution (Choropleth Map)\n",
        "    2. Top affiliations (Bar Chart)\n",
        "    3. Yearly publication trends (Time Series) - Now using paper data\n",
        "    \"\"\"\n",
        "\n",
        "    df_authors = pd.DataFrame(author_data)\n",
        "    df_papers = pd.DataFrame(paper_data)\n",
        "\n",
        "    ## **World Map (Choropleth) - Country Distribution**\n",
        "    country_counts = df_authors[\"country\"].value_counts().reset_index()\n",
        "    country_counts.columns = [\"country\", \"count\"]  # Rename columns correctly\n",
        "\n",
        "    fig1 = px.choropleth(\n",
        "        country_counts,\n",
        "        locations=\"country\",\n",
        "        locationmode=\"country names\",\n",
        "        color=\"count\",\n",
        "        title=\"Researcher Distribution by Country\",\n",
        "        color_continuous_scale=\"Blues\"\n",
        "    )\n",
        "\n",
        "    ## **Bar Chart - Top 10 Affiliations**\n",
        "    affiliation_counts = df_authors[\"affiliation\"].value_counts().nlargest(10).reset_index()\n",
        "    affiliation_counts.columns = [\"affiliation\", \"count\"]\n",
        "\n",
        "    fig2 = px.bar(\n",
        "        affiliation_counts,\n",
        "        x=\"affiliation\",\n",
        "        y=\"count\",\n",
        "        labels={\"affiliation\": \"Institution\", \"count\": \"Number of Researchers\"},\n",
        "        title=\"Top 10 Research Institutions\"\n",
        "    )\n",
        "\n",
        "    ## **Time Series - Yearly Publication Trends**\n",
        "    year_counts = df_papers[\"year\"].value_counts().sort_index().reset_index()\n",
        "    year_counts.columns = [\"year\", \"count\"]  # Fix column naming\n",
        "\n",
        "    fig3 = px.line(\n",
        "        year_counts,\n",
        "        x=\"year\",\n",
        "        y=\"count\",\n",
        "        labels={\"year\": \"Year\", \"count\": \"Papers Published\"},\n",
        "        title=\"Yearly Publication Trends\"\n",
        "    )\n",
        "\n",
        "    # Return the JSON representations of the plots\n",
        "    return {\n",
        "        \"plots\": {\n",
        "            \"world_map\": fig1.to_json(),\n",
        "            \"affiliations\": fig2.to_json(),\n",
        "            \"yearly_trends\": fig3.to_json()\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Tool: Find most influential authors\n",
        "def find_influential_authors():\n",
        "    \"\"\"\n",
        "    Finds the most influential authors in the network using the PageRank algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Construct the graph from ArangoDB\n",
        "    query = aql = \"\"\"\n",
        "      LET edges = (\n",
        "        FOR edge IN co_author_with\n",
        "          RETURN edge\n",
        "      )\n",
        "      LET nodes = (\n",
        "        FOR edge IN edges\n",
        "          FOR nodeId IN [edge._from, edge._to]\n",
        "            COLLECT uniqueId = nodeId INTO nodesGrouped\n",
        "            RETURN DOCUMENT(uniqueId)\n",
        "      )\n",
        "      RETURN { nodes, edges }\n",
        "      \"\"\"\n",
        "    results = list(db.aql.execute(query))\n",
        "    cursor = db.aql.execute(aql)\n",
        "    result = cursor.next()\n",
        "\n",
        "    # Make the networkX graph\n",
        "    # Have to do it this way, as\n",
        "    # G_adb does not support subgraphs\n",
        "    G = nx.Graph()\n",
        "    for node in result[\"nodes\"]:\n",
        "        G.add_node(node[\"_key\"], **node)\n",
        "    for edge in result[\"edges\"]:\n",
        "        source_key = edge[\"_from\"].split(\"/\")[1]\n",
        "        target_key = edge[\"_to\"].split(\"/\")[1]\n",
        "        G.add_edge(source_key, target_key, weight=edge.get(\"weight\", 1))\n",
        "\n",
        "    pagerank_scores = nx.pagerank(G)\n",
        "    top_10_authors = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    top_authors_data = [\n",
        "      {\n",
        "          \"name\": G.nodes[author].get(\"name\"),\n",
        "          \"gender\": G.nodes[author].get(\"gender\"),\n",
        "          \"country\": G.nodes[author].get(\"country\"),\n",
        "          \"affiliation\": G.nodes[author].get(\"affiliation\"),\n",
        "          \"pagerank\": score\n",
        "      }\n",
        "      for author, score in top_10_authors\n",
        "    ]\n",
        "\n",
        "    top_10_authors_df = pd.DataFrame(top_authors_data)\n",
        "    return top_10_authors_df"
      ],
      "metadata": {
        "id": "pcMA4l27LoeA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dashboard\n",
        "The following is a nice (?) interactive dashboard that offers a mix of free-form queries, full-text, AQL and cuGraph queries"
      ],
      "metadata": {
        "id": "k0Bqh2wToqN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import plotly.io as pio\n",
        "\n",
        "# just a convenience function\n",
        "def agent_query(question: str) -> str:\n",
        "    answer = query_graph(question)\n",
        "    return answer\n",
        "\n",
        "def generate_dashboard(user_query):\n",
        "    \"\"\"Fetches the response and extracts Plotly figures in one step.\"\"\"\n",
        "    papers = search_papers(user_query)\n",
        "    #print(papers)\n",
        "    paper_ids = [paper[\"id\"] for paper in papers]\n",
        "\n",
        "    authors = get_authors_for_papers(paper_ids)\n",
        "    #print(authors)\n",
        "\n",
        "    plot_dict = generate_plots(authors, papers)\n",
        "    plotly_plots = plot_dict[\"plots\"]\n",
        "    #print(plotly_plots)\n",
        "\n",
        "\n",
        "    figs = []\n",
        "\n",
        "    # Try to extract Plotly figures\n",
        "    try:\n",
        "      for key, fig_json in plotly_plots.items():\n",
        "          fig = pio.from_json(fig_json)\n",
        "          figs.append(fig)\n",
        "    except:\n",
        "      # Don't crash if the plots are not present\n",
        "      pass\n",
        "\n",
        "    return tuple(figs)\n",
        "\n",
        "# Create Gradio app\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"Full-Text Abstracts Search Dashboard\")\n",
        "    with gr.Row():\n",
        "      free_text_input = gr.Textbox(label=\"Enter your question\")\n",
        "      free_text_btn = gr.Button(\">>\")\n",
        "      free_text_output = gr.Textbox(label=\"Answer\")\n",
        "    free_text_btn.click(\n",
        "      agent_query,\n",
        "      inputs=free_text_input,\n",
        "      outputs=free_text_output\n",
        "    )\n",
        "\n",
        "    # Just a static dataframe\n",
        "    gr.Markdown(\"influential Authors\")\n",
        "    gr.DataFrame(find_influential_authors())\n",
        "\n",
        "    gr.Markdown(\"Full-Text Abstracts Search Dashboard\")\n",
        "    user_query = gr.Textbox(label=\"Enter a topic for searching within Abstracts\")\n",
        "    execute_btn = gr.Button(\"Submit\")\n",
        "\n",
        "    # Visualisation placeholders\n",
        "    plot1 = gr.Plot()\n",
        "    plot2 = gr.Plot()\n",
        "    plot3 = gr.Plot()\n",
        "\n",
        "    execute_btn.click(\n",
        "        generate_dashboard,\n",
        "        inputs=user_query,\n",
        "        outputs=[plot1, plot2, plot3]\n",
        "    )\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(share=True, inbrowser=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "MSp6-mh2LBaC",
        "outputId": "5e38d914-acaa-43e2-ca1f-079d51cd8434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a8eb0ab6e90429762b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a8eb0ab6e90429762b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
            "AQL Query (1):\u001b[32;1m\u001b[1;3m\n",
            "WITH Paper, Author, written_by\n",
            "FOR author IN Author\n",
            "  FILTER author.name == \"Terence Tao\"\n",
            "  FOR paper IN INBOUND author written_by\n",
            "    RETURN paper\n",
            "\u001b[0m\n",
            "AQL Result:\n",
            "\u001b[32;1m\u001b[1;3m[{'_key': '10_1002_CPA_20124', '_id': 'Paper/10_1002_CPA_20124', '_rev': '_jVwbTwu--h', 'type': 'Paper', 'title': 'Sparse Signal Recovery via L1-Regularization and Matrix Uncertainty', 'year': 2005, 'num_citations': 6257, 'abstract': \"Suppose we wish to recover a vector x_0 –Ñ R^m (e.g., a digital signal or image) from incomplete and contaminated observations y = Ax_0 + e; A is an n by m matrix with far fewer rows than columns (n ¬´ m) and e is an error term. Is it possible to recover x_0 accurately based on the data y? \\nTo recover x_0, we consider the solution x^# to the l_(1-)regularization problem min ‚Äñx‚Äñl_1 subject to ‚ÄñAx - y‚Äñl(2) ‚â§ –Ñ, where –Ñ is the size of the error term e. We show that if A obeys a uniform uncertainty principle (with unit-normed columns) and if the vector x_0 is sufficiently sparse, then the solution is within the noise level ‚Äñx^# - x_0‚Äñl_2 ‚â§ C –Ñ. As a first example, suppose that A is a Gaussian random matrix; then stable recovery occurs for almost all such A's provided that the number of nonzeros of x_0 is of about the same order as the number of observations. As a second instance, suppose one observes few Fourier samples of x_0; then stable recovery occurs for almost any set of n coefficients provided that the number of nonzeros is of the order of n/[log m]^6. In the case where the error term vanishes, the recovery is of course exact, and this work actually provides novel insights into the exact recovery phenomenon discussed in earlier papers. The methodology also explains why one can also very nearly recover approximately sparse signals.\", 'country': 'UNKNOWN', 'gender': 'male'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c0422e3e07b4921ae79bc629d69bfc1": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_af9e3e491ee1457f9ca06b81b8cb8c33",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "(NX ‚Üí ADB): Nodes \u001b[38;2;151;196;35m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[35m100%\u001b[0m (2840/2840) \u001b[33m0:00:00\u001b[0m\n     ADB Import: 'Author' (1740) \u001b[38;2;91;192;222m‚ñ∞‚ñ∞‚ñ∞‚ñ∞‚ñ±‚ñ±‚ñ±\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">(NX ‚Üí ADB): Nodes <span style=\"color: #97c423; text-decoration-color: #97c423\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> (2840/2840) <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n     ADB Import: 'Author' (1740) <span style=\"color: #5bc0de; text-decoration-color: #5bc0de\">‚ñ∞‚ñ∞‚ñ∞‚ñ∞‚ñ±‚ñ±‚ñ±</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "af9e3e491ee1457f9ca06b81b8cb8c33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae5e833f4874511a8cdd1b1b3522873": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_97664723412c475c82d0cc663aee079b",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "(NX ‚Üí ADB): Edges \u001b[38;2;94;49;8m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[35m100%\u001b[0m (5484/5484) \u001b[33m0:00:00\u001b[0m\n     ADB Import: 'co_author_with' (3131) \u001b[38;2;91;192;222m‚ñ∞‚ñ∞‚ñ∞‚ñ±‚ñ±‚ñ±‚ñ±\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">(NX ‚Üí ADB): Edges <span style=\"color: #5e3108; text-decoration-color: #5e3108\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> (5484/5484) <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n     ADB Import: 'co_author_with' (3131) <span style=\"color: #5bc0de; text-decoration-color: #5bc0de\">‚ñ∞‚ñ∞‚ñ∞‚ñ±‚ñ±‚ñ±‚ñ±</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "97664723412c475c82d0cc663aee079b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}